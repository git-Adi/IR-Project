{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/group36/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/group36/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/group36/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import load\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import math\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm  \n",
    "import faiss\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45548/1285203385.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_mapping['Company Namext'] = stock_mapping['Company Name'].str.replace('Ltd.', '')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"ind_nifty50list.csv\")\n",
    "stock_mapping = data[['Symbol', \"Company Name\"]]\n",
    "stock_mapping['Company Namext'] = stock_mapping['Company Name'].str.replace('Ltd.', '')\n",
    "\n",
    "embedding_data = pd.read_pickle(\"Bert_Embeddings.pkl\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval() \n",
    "\n",
    "\n",
    "embeddings = np.array(embedding_data['bert_embeddings'].tolist()).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1]) \n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbol = \"\"\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    embeddings = output.last_hidden_state.mean(1)\n",
    "    return embeddings.squeeze().numpy()\n",
    "\n",
    "\n",
    "# get chunks from the data according to maximum cosine similarity\n",
    "def get_chunks_symbol(query_vector, stock_data):\n",
    "    scores = []\n",
    "\n",
    "    for _, row in stock_data.iterrows():\n",
    "        scores.append([[row['chunks'], row['tables'], row['type']], cosine_similarity([row['TF-IDF']], query_vector)[0][0]])\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_chunks = []\n",
    "    for i in range(3):\n",
    "        top_chunks.append(scores[i][0])\n",
    "    return top_chunks\n",
    "\n",
    "\n",
    "# get chunks \n",
    "def get_chunks_without_symbol(query):\n",
    "    query_vector = get_bert_embedding(\" \".join(preprocess_text(query)))\n",
    "\n",
    "    # now here we can use cosine similarity or faiss model\n",
    "    k = 3\n",
    "    distances, indices = index.search(np.expand_dims(query_vector, axis=0), k)\n",
    "\n",
    "    top_chunks = embedding_data.iloc[indices[0]]\n",
    "    return top_chunks\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def process_head(query):\n",
    "    symbol = \"\"\n",
    "    \n",
    "    for _, row in stock_mapping.iterrows():\n",
    "        if(row['Symbol'] in query  or row['Company Name'] in query or row['Company Namext'] in query):\n",
    "            symbol = row['Symbol']\n",
    "            break\n",
    "    global stock_symbol\n",
    "    stock_symbol = symbol\n",
    "    \n",
    "    \n",
    "    if(symbol != \"\"):\n",
    "        with open(f'vectorizer/{symbol}_vectorizer.pkl', 'rb') as f:\n",
    "            tfidf_vectorizer = pickle.load(f)\n",
    "        stocks_data = pd.read_pickle(f\"fstocks_data/{symbol}_data.pkl\")\n",
    "        query_vector = tfidf_vectorizer.transform([\" \".join(preprocess_text(query))]).toarray()\n",
    "        \n",
    "        return get_chunks_symbol(query_vector, stocks_data)\n",
    "    \n",
    "    return get_chunks_without_symbol(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries\n",
    "top_chunks = process_head(\"how ITC is contributing to employment generation\")\n",
    "top_chunks = process_head(\"how ITC is contributing to amrit kaal\")\n",
    "top_chunks = process_head(\"In which sectors ADANIENT invests?\")\n",
    "\n",
    "query = \"What is the net profit of ITC company for this yaer ? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>StockName</th>\n",
       "      <th>Year</th>\n",
       "      <th>PDF_Path</th>\n",
       "      <th>tables</th>\n",
       "      <th>type</th>\n",
       "      <th>bert_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78568</th>\n",
       "      <td>management estimate long-term compound annual ...</td>\n",
       "      <td>Tech Mahindra Limited</td>\n",
       "      <td>TECHM</td>\n",
       "      <td>2022-2023</td>\n",
       "      <td>extracted_pdfs/AR_22069_TECHM_2022_2023_260620...</td>\n",
       "      <td>output_data/data_22651.txt</td>\n",
       "      <td>Financial Statements</td>\n",
       "      <td>[-0.33832258, -0.54165375, 0.34757358, 0.11431...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43921</th>\n",
       "      <td>derivative retail institutional market share e...</td>\n",
       "      <td>Kotak Mahindra Bank Limited</td>\n",
       "      <td>KOTAKBANK</td>\n",
       "      <td>2022-2023</td>\n",
       "      <td>extracted_pdfs/AR_2022_2023.pdf</td>\n",
       "      <td>output_data/data_12363.txt</td>\n",
       "      <td>Financial Statements</td>\n",
       "      <td>[0.33105066, -0.0840581, 0.20947589, -0.094901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12180</th>\n",
       "      <td>derivative retail institutional market share e...</td>\n",
       "      <td>Bajaj Finserv Limited</td>\n",
       "      <td>BAJAJFINSV</td>\n",
       "      <td>2022-2023</td>\n",
       "      <td>extracted_pdfs/AR_2022_2023.pdf</td>\n",
       "      <td>output_data/data_3650.txt</td>\n",
       "      <td>Financial Statements</td>\n",
       "      <td>[0.33105066, -0.0840581, 0.20947589, -0.094901...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  chunks  \\\n",
       "78568  management estimate long-term compound annual ...   \n",
       "43921  derivative retail institutional market share e...   \n",
       "12180  derivative retail institutional market share e...   \n",
       "\n",
       "                       CompanyName   StockName       Year  \\\n",
       "78568        Tech Mahindra Limited       TECHM  2022-2023   \n",
       "43921  Kotak Mahindra Bank Limited   KOTAKBANK  2022-2023   \n",
       "12180        Bajaj Finserv Limited  BAJAJFINSV  2022-2023   \n",
       "\n",
       "                                                PDF_Path  \\\n",
       "78568  extracted_pdfs/AR_22069_TECHM_2022_2023_260620...   \n",
       "43921                    extracted_pdfs/AR_2022_2023.pdf   \n",
       "12180                    extracted_pdfs/AR_2022_2023.pdf   \n",
       "\n",
       "                           tables                  type  \\\n",
       "78568  output_data/data_22651.txt  Financial Statements   \n",
       "43921  output_data/data_12363.txt  Financial Statements   \n",
       "12180   output_data/data_3650.txt  Financial Statements   \n",
       "\n",
       "                                         bert_embeddings  \n",
       "78568  [-0.33832258, -0.54165375, 0.34757358, 0.11431...  \n",
       "43921  [0.33105066, -0.0840581, 0.20947589, -0.094901...  \n",
       "12180  [0.33105066, -0.0840581, 0.20947589, -0.094901...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"among nifty 50 which stock has highest market cap\"\n",
    "# query = \"What is the EBITDA of ITC company ? Give me from financial statements of company   EBITDA - Earnings before interest tax Depreciation Ammortization\"\n",
    "query = \"Which comapny has largest market cap\"\n",
    "top_chunks = process_head(query)\n",
    "\n",
    "top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78568    management estimate long-term compound annual ...\n",
       "43921    derivative retail institutional market share e...\n",
       "12180    derivative retail institutional market share e...\n",
       "Name: chunks, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_chunks = top_chunks['chunks']\n",
    "top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'management estimate long-term compound annual ebitda growth rate consistent assumption market participant would make \\n\\n derivative retail institutional market share excluding bse derivative proprietary segment net npa customer asset st march business overview \\n\\n derivative retail institutional market share excluding bse derivative proprietary segment net npa customer asset st march business overview \\n\\n '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = \"\"\n",
    "for chunk in top_chunks:\n",
    "    output += chunk + \" \\n\\n \"\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  [Errno 2] No such file or directory: 'preprocess/output_data/data_11272.txt'\n",
      "INDEPENDENT AUDITOR’S REPORT\t\t\n",
      "To the Members of ITC Limited\t\t\n",
      "Report on the Audit of the Standalone Ind AS Financial\tInstitute of Chartered Accountants of India together with\t\n",
      "Statements\tthe  ethical  requirements  that  are  relevant  to  our  audit\t\n",
      "\tof  the  financial  statements  under  the  provisions  of  the\t\n",
      "Opinion\t\t\n",
      "\tAct  and  the  Rules  thereunder,  and  we  have  fulfilled  our\t\n",
      "We  have  audited  the  accompanying  standalone  Ind  AS\t\t\n",
      "\tother  ethical  responsibilities \n",
      "in  accordance  with \n",
      "these\t\n",
      "financial statements of ITC Limited (“the Company”), which\t\t\n",
      "\trequirements  and  the  Code  of  Ethics.  We  believe  that\t\n",
      "comprise  the  Balance  Sheet  as  at  March  31,  2023,  the\t\t\n",
      "\tthe  audit  evidence  we  have  obtained  is  sufficient  and\t\n",
      "Statement of Profit and Loss, including  the statement of\t\t\n",
      "\tappropriate to provide a basis for our audit opinion on the\t\n",
      "Other Comprehensive Income, the Cash Flow Statement\t\t\n",
      "\tstandalone Ind AS financial statements.\t\n",
      "and  the  Statement  of  Changes  in  Equity  for  the  year\t\t\n",
      "\tKey Audit Matters\t\n",
      "then ended, and notes to the standalone Ind AS financial\t\t\n",
      "statements, including a summary of significant accounting\tKey audit matters are those matters that, in our professional\t\n",
      "policies and other explanatory information.\tjudgement,  were  of  most  significance  in  our  audit  of  the\t\n",
      "\tstandalone Ind AS financial statements for the financial year\t\n",
      "In  our  opinion  and  to  the  best  of  our  information  and\t\t\n",
      "\tended March 31, 2023. These matters were addressed in\t\n",
      "according  to  the  explanations  given  to  us,  the  aforesaid\t\t\n",
      "\tthe context of our audit of the standalone Ind AS financial\t\n",
      "standalone Ind AS financial statements give the information\t\t\n",
      "\tstatements as a whole, and in forming our opinion thereon,\t\n",
      "required  by \n",
      "the  Companies  Act,  2013,  as  amended\t\t\n",
      "\tand we do not provide a separate opinion on these matters.\t\n",
      "(“the  Act”)  in  the  manner  so  required  and  give  a  true\t\t\n",
      "\tFor  each  matter  below,  our  description  of  how  our  audit\t\n",
      "and fair view in conformity with the accounting principles\t\t\n",
      "\taddressed the matter is provided in that context.\t\n",
      "generally  accepted  in  India,  of  the  state  of  affairs  of  the\t\t\n",
      "\tWe  have  determined  the  matters  described  below  to  be\t\n",
      "Company as at March 31, 2023, its profit including other\t\t\n",
      "\tthe key audit matters to be communicated in our report.\t\n",
      "comprehensive income, its cash flows and the changes in\t\t\n",
      "\tWe  have \n",
      "fulfilled \n",
      "the  responsibilities  described \n",
      "in \n",
      "the\t\n",
      "equity for the year ended on that date.\t\t\n",
      "\tAuditor’s  responsibilities  for  the  audit  of  the  standalone\t\n",
      "Basis for Opinion\t\t\n",
      "\tInd AS financial statements section of our report, including\t\n",
      "We conducted our audit of the standalone Ind AS financial\tin relation to these matters. Accordingly, our audit included\t\n",
      "statements in accordance with the Standards on Auditing\tthe  performance  of  procedures  designed  to  respond  to\t\n",
      "(SAs),  as  specified  under  section  143(10)  of \n",
      "the  Act.\tour  assessment  of  the  risks  of  material  misstatement  of\t\n",
      "Our  responsibilities  under  those  Standards  are  further\tthe standalone Ind AS financial statements. The results of\t\n",
      "described  in  the  ‘Auditor’s  Responsibilities  for  the  Audit\tour audit procedures, including the procedures performed\t\n",
      "of  the  Standalone  Ind  AS  Financial  Statements’  section\tto  address  the  matters  below,  provide  the  basis  for  our\t\n",
      "of  our \n",
      "report.  We  are \n",
      "independent  of \n",
      "the  Company\taudit  opinion  on  the  accompanying  standalone  Ind  AS\t\n",
      "in  accordance  with  the  ‘Code  of  Ethics’  issued  by  the\tfinancial statements.\t\n",
      "Key audit matters\tHow our audit addressed the key audit matter\t\n",
      "Revenue recognition\t\t\n",
      "Revenue  from  the  sale  of  goods  (hereinafter  referred\tOur audit procedures included the following:\t\n",
      "to  as  “Revenue”) \n",
      "is  recognised  when \n",
      "the  Company\t\t\n",
      "\tAssessed \n",
      "the \n",
      "Company’s \n",
      "revenue \n",
      "recognition\t\n",
      "performs its obligation to its customers and the amount\t\t\n",
      "\taccounting policies in line with Ind AS 115 (“Revenue\t\n",
      "of revenue can be measured reliably and recovery of the\t\t\n",
      "\tfrom Contracts with Customers”) and tested thereof.\t\n",
      "consideration  is  probable.  The  timing  of  such  revenue\t\t\n",
      "\tEvaluated \n",
      "the \n",
      "integrity  of \n",
      "the  general \n",
      "information\t\n",
      "recognition in case of sale of goods is when the control\t\t\n",
      "\tand  technology  control  environment  and  testing  the\t\n",
      "over the same is transferred to the customer, which is\t\t\n",
      "\toperating effectiveness of key IT application controls\t\n",
      "mainly upon delivery.\t\t\n",
      "\tover recognition of revenue.\t\n",
      "The \n",
      "timing  of \n",
      "revenue \n",
      "recognition \n",
      "is \n",
      "relevant \n",
      "to\t\t\n",
      "\tEvaluated the design, implementation and operating\t\n",
      "the \n",
      "reported \n",
      "performance \n",
      "of \n",
      "the  Company.  The\t\t\n",
      "\teffectiveness  of  Company’s  controls \n",
      "in  respect  of\t\n",
      "management considers revenue as a key measure for\t\t\n",
      "\trevenue recognition.\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "31. Financial Instruments and Related Disclosures (Contd.)\t\n",
      "Derivatives are valued using valuation techniques with market observable inputs such as foreign exchange spot rates and forward\t\n",
      "rates  at  the  end  of  the  reporting  period,  yield  curves,  risk  free  rate  of  returns,  volatility  etc.,  as  applicable.  The  fair  value  of\t\n",
      "investment in Bonds / Debentures, Certificate of Deposits, Venture Capital funds etc. and financial liabilities, where applicable, is\t\n",
      "determined using market observable inputs such as quotes from market participants, value published by the issuer etc.\t\n",
      "Level 3: Inputs for the assets or liabilities that are not based on observable market data (unobservable inputs).\t\n",
      "If  one  or  more  of  the  significant  inputs  is  not  based  on  observable  market  data,  the  fair  value  is  determined  using  generally\t\n",
      "accepted  methodologies  such  as  discounted  cash  flow  analysis,  with  the  most  significant  inputs  being  the  discount  rate  that\t\n",
      "reflects the credit risk of counterparty.\t\n",
      "The fair value of trade receivables, trade payables and other Current financial assets and liabilities is considered to be equal to the\t\n",
      "carrying amounts of these items due to their short-term nature. Where such items are Non-current in nature, the same has been\t\n",
      "classified as Level 3 and fair value determined using discounted cash flow basis. Similarly, unquoted equity instruments where\t\n",
      "most recent information to measure fair value is insufficient, or if there is a wide range of possible fair value measurements, cost\t\n",
      "has been considered as best estimate of fair value.\t\n",
      "There has been no change in the valuation methodology for Level 3 inputs during the year. The Group has not classified any\t\n",
      "material financial instruments under Level 3 of the fair value hierarchy. The sensitivity of change in the unobservable inputs used\t\n",
      "in fair valuation of Level 3 financial assets and liabilities does not have a significant impact on their value. There were no transfers\t\n",
      "between Level 1, Level 2 and Level 3 during the year.\t\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables = set()\n",
    "for chunk in top_chunks:\n",
    "    for table in chunk[1].split(\",\"):\n",
    "        tables.add(table)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "table_content = \"\"\n",
    "for table in tables:\n",
    "    file_path = \"preprocess/\" + table\n",
    "    try:\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            file_content = f.read()\n",
    "            print(file_content)\n",
    "            print(\"\\n\\n\")\n",
    "            \n",
    "            count += 1\n",
    "            table_content += file_content + \"\\n\\n\\n\"\n",
    "    \n",
    "        if(count == 2):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fundamentals = \"\"\n",
    "\n",
    "with open(f\"minimised_tables/{stock_symbol}_tables.txt\", 'r') as f:\n",
    "    fundamentals_data = f.read().lower()\n",
    "    fundamentals_data = fundamentals_data.replace(\"\\n\", \" \\n \")\n",
    "\n",
    "for token in preprocess_text(query):\n",
    "    if(token in fundamentals_data):\n",
    "        fundamentals = fundamentals_data\n",
    "        break\n",
    "\n",
    "fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crisis livelihood creation itc embarked even ambitious sustainability journey raise bar create secure better tomorrow following page provide glimpse itc multi-dimensional growth driver propelling company new horizon competitive sustainable progress contributing india journey amrit kaal \\n\\n ',\n",
       " \"certificate standalone financial statement balance sheet statement profit loss statement change equity cash flow statement note financial statement independent auditor report guide subsidiary joint venture associate salient feature financial statement subsidiary joint venture associate form aoc- consolidated financial statement ten year glance financial highlight business responsibility sustainability report i-xlviii itc infotech business friendly solution creating enduring institution award accoladesreport account content itc contributing india amrit kaal creating value agriculture manufacturing servicescontents hyper-linked relevant page report click 'itc limited headerfooter page return content page \\n\\n \",\n",
       " 'report account creating value agriculture manufacturing servicesitc contributing india amrit kaal inspired credo nation first sab saath badhein itc pioneered paradigm responsible competitiveness focus building extreme competitiveness business whilst serving national priority generating sustainable livelihood enriching environment itc contributes nation building unleashing multiple driver growth manifest growing presence across three sector economy agriculture manufacturing service creation world-class indian brand investment creating state-of-the-art manufacturing asset building iconic hospitality asset empowering farmer well rural community endeavour create enduring value stakeholder nation today world traversing unprecedented challenge emerging polycrisis encompassing climate change widening inequality geo-political issue reglobalisation new global economic order shaped crisis \\n\\n ',\n",
       " 'structural competitiveness create new vitality enterprise strategic intervention make growing contribution nation journey amrit kaal core element itc next itc exploring opportunity craft disruptive business model anchored intersection two mega trend digital sustainability leveraging company institutional strength enabled itc unleash new vector growth intervention like itcmaars phygital farmer empowerment ecosystem food tech service sustainable packaging addition new driver growth also pursued value accretive acquisition export itc diversiﬁed portfolio future-ready business spanning fmcg paperboard packaging agri business hotel information technology enables contribute meaningfully sector indian economy infuse new energy growth driver future signiﬁcant investment made across building asset accelerating digital transformation \\n\\n ',\n",
       " 'well rural community endeavour create enduring value stakeholder nation today world traversing unprecedented challenge emerging polycrisis encompassing climate change widening inequality geo-political issue reglobalisation new global economic order shaped crisis spurring new strategy innovation business model addressing challenge opportunity itc crafted comprehensiveitc next vision build future-tech innovative climate positive sustainable inclusive enterprise powered agility competitiveness resilience consumer centricity new avenue growth identiﬁed whilst fortifying existing business putting place higher order structural competitiveness create new vitality enterprise strategic intervention make growing contribution nation journey amrit kaal core element itc next itc exploring opportunity craft disruptive business model anchored intersection two mega \\n\\n ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.46s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/group36/ir_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:4\n"
     ]
    }
   ],
   "source": [
    "# here we will load Llama model and pass in the chunks with the query\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the bitsandbytes library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=hf_auth\n",
    ")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_auth, truncation=True)\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",  # task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=200, \n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer1.eos_token_id,\n",
    "    use_auth_token=hf_auth  # Passing token to pipeline\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which comapny has largest market cap\n",
      "\n",
      "Considering the following contexts only and answer from the provided context only:\n",
      "\n",
      "management estimate long-term compound annual ebitda growth rate consistent assumption market participant would make \n",
      "\n",
      " derivative retail institutional market share excluding bse derivative proprietary segment net npa customer asset st march business overview \n",
      "\n",
      " derivative retail institutional market share excluding bse derivative proprietary segment net npa customer asset st march business overview \n",
      "\n",
      " \n",
      "\n",
      "Some fundamentals about the stock:\n",
      "\n",
      "\n",
      "Answer:\n",
      "Answer your question using the provided contexts only . The Answer should be conscise, short and to the point. You should answer from the contexts provided to you only. \n",
      "If the contexts are not suffcient to answer the question, respond with:\n",
      "\"I don't have enough information to answer your question.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text based on a prompt\n",
    "\n",
    "# files needed to be sended for fundamentals -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# template = f\"\"\"Question: {query}\n",
    "\n",
    "# Considering the following contexts only:\n",
    "\n",
    "# {\" \".join(top_chunks)}\n",
    "# Answer:\n",
    "# Answer your question using the provided contexts only . The Answer should be conscise, short and to the point. You should answer from the contexts provided to you only. \n",
    "# If the contexts are not sufficient to answer the question, respond with:\n",
    "# \"I don't have enough information to answer your question.\"\n",
    "# \"\"\"\n",
    "\n",
    "template = f\"\"\"Question: {query}\n",
    "\n",
    "Considering the following contexts only and answer from the provided context only:\n",
    "\n",
    "{output}\n",
    "\n",
    "Some fundamentals about the stock:\n",
    "{fundamentals}\n",
    "\n",
    "Answer:\n",
    "Answer your question using the provided contexts only . The Answer should be conscise, short and to the point. You should answer from the contexts provided to you only. \n",
    "If the contexts are not suffcient to answer the question, respond with:\n",
    "\"I don't have enough information to answer your question.\"\n",
    "\"\"\"\n",
    "prompt = template\n",
    "print(prompt)\n",
    "\n",
    "generated_text = text_generator(prompt, max_length=40096, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: how ITC is contributing to employment generation\\n\\nConsidering the following contexts only:\\n\\nscheme sector expected play critical role boosting investment agri export farmer income employment generation building indian brand global market company included scheme several food business well agri business \\n\\n fatality reported essential indicator rehabilitated placed suitable employment whose family member placed suitable employment xxvitotal affected employeesworkers employeesworkers rehabilitated placed suitable employment whose family member placed suitable employment fy fy fy fy employee worker onsite accident entity pr ovide transition assistance programme facilitate continued employability management career ending resulting retirement termination employment yesno itc continually invests human capital development includes building skill capability contemporary providing employee diversity experience enhance employability workforce enable smooth transition alternate opportunity sought company place programme called making new choice retiring staff addition company provides pension benefit post-retiral medical benefit member staff qualify worker provided pension \\n\\n national bamboo mission nodal agency state government cultivating bamboo plantation country proactive measure implemented company highlighted sub-serve national priority employment generation provide source competitive advantage business contributing towards enhancing income bamboo farmer agarbatti stick raw batti manufacturing value chain safety match industry business strengthened market leadership position leveraging brand homelites built differentiated positioning stronger longer karborised stick business continues focus scaling share value-added product portfolio enhancing supply chain efficiency sourcing product manufactured closer market trade marketing distribution company trade marketing distribution tm vertical demonstrated remarkable agility responsiveness capitalising emergent market opportunity arising normalisation market condition premiumisation increasing online purchase concerted \\n\\n \\n\\nSome fundamentals about the stock:\\n\\n\\nAnswer:\\nAnswer your question using the provided contexts only . The Answer should be conscise, short and to the point. You should answer from the contexts provided to you only. \\nIf the contexts are not suffcient to answer the question, respond with:\\n\"I don\\'t have enough information to answer your question.\"\\n\\nPlease provide the question you want me to answer.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nWhich company has the largest market capitalization?\\n\\nBased on the provided context, I can determine that the company\\'s market capitalization is approximately $100 billion. However, I don\\'t have enough information to identify the specific company with the largest market capitalization.\\n\\nPlease provide additional context or clarify your question to help me narrow down the answer.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_te = generated_text[0]['generated_text']\n",
    "index = generated_te.find(\"I don't have enough information to answer your question.\") + 56\n",
    "\n",
    "generated_te[index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "Which company has the largest market capitalization?\n",
      "\n",
      "Based on the provided context, I can determine that the company's market capitalization is approximately $100 billion. However, I don't have enough information to identify the specific company with the largest market capitalization.\n",
      "\n",
      "Please provide additional context or clarify your question to help me narrow down the answer.\n"
     ]
    }
   ],
   "source": [
    "print(generated_te[index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9648"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:15<00:00, 21.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def convert_tfidf_string_to_array(tfidf_str):\n",
    "    return eval(tfidf_str)\n",
    "\n",
    "csv_folder = \"stocks_data\"\n",
    "\n",
    "pickle_folder = \"fstocks_data\"\n",
    "\n",
    "for csv_file in tqdm(os.listdir(csv_folder)):\n",
    "    if csv_file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(csv_folder, csv_file))\n",
    "        \n",
    "        df['TF-IDF'] = df['TF-IDF'].apply(convert_tfidf_string_to_array)\n",
    "        \n",
    "        df.to_pickle(os.path.join(pickle_folder, csv_file.split(\".\")[0]+\".pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 23 15:37:23 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    15W / 250W |   2095MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    14W / 250W |    569MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    23W / 250W |    569MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    17W / 250W |    569MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 30%   25C    P8    18W / 250W |    569MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     27880      C   ...da3/envs/llma1/bin/python     2092MiB |\n",
      "|    1   N/A  N/A     27880      C   ...da3/envs/llma1/bin/python      566MiB |\n",
      "|    2   N/A  N/A     27880      C   ...da3/envs/llma1/bin/python      566MiB |\n",
      "|    3   N/A  N/A     27880      C   ...da3/envs/llma1/bin/python      566MiB |\n",
      "|    4   N/A  N/A     27880      C   ...da3/envs/llma1/bin/python      566MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
